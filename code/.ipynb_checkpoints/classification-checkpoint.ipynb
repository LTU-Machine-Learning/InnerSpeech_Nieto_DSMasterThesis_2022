{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awscli in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (1.24.5)\r\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (0.16)\r\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (5.4.1)\r\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (4.7.2)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (0.5.2)\r\n",
      "Requirement already satisfied: botocore==1.26.5 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (1.26.5)\r\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from awscli) (0.4.4)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from botocore==1.26.5->awscli) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from botocore==1.26.5->awscli) (1.26.9)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from botocore==1.26.5->awscli) (1.0.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.26.5->awscli) (1.16.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\r\n"
     ]
    }
   ],
   "source": [
    "# Initialize the libs and download the dataset\n",
    "!pip3 install mne -q\n",
    "!pip3 install awscli\n",
    "!aws s3 sync --no-sign-request s3://openneuro.org/ds003626 ds003626/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(23)\n",
    "\n",
    "mne.set_log_level(verbose='warning') #to avoid info at terminal\n",
    "warnings.filterwarnings(action = \"ignore\", category = DeprecationWarning )\n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The root dir\n",
    "root_dir = \"./ds003626\"\n",
    "\n",
    "# Sampling rate\n",
    "fs = 256\n",
    "\n",
    "# Select the useful par of each trial. Time in seconds\n",
    "t_start = 1.5\n",
    "t_end = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from aux.pre_process import get_subjects_data_and_label\n",
    "\n",
    "condition = \"Inner\"\n",
    "\n",
    "data, labels = get_subjects_data_and_label(root_dir, condition, t_start = t_start, t_end = t_end, fs = fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects 10\n",
      "Data shape: [trials x channels x samples]\n",
      "Shape (200, 128, 512)\n",
      "Labels\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of subjects\", len(data))\n",
    "print(\"Data shape: [trials x channels x samples]\")\n",
    "print(\"Shape\", data[0].shape) # Trials, channels, samples\n",
    "\n",
    "print(\"Labels\")\n",
    "print(len(labels)) # Time stamp, class , condition, session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2236, 128, 512) (2236,)\n"
     ]
    }
   ],
   "source": [
    "data_array=np.vstack(data)\n",
    "label_array=np.hstack(labels)\n",
    "print(data_array.shape, label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define all the features\n",
    "from scipy import stats\n",
    "\n",
    "def mean(x):\n",
    "    return np.mean(x, axis=-1)\n",
    "\n",
    "def std(x):\n",
    "    return np.std(x, axis=-1)\n",
    "\n",
    "def ptp(x):\n",
    "    return np.ptp(x, axis=-1)\n",
    "\n",
    "def var(x):\n",
    "    return np.var(x, axis=-1)\n",
    "\n",
    "def minim(x):\n",
    "    return np.min(x, axis=-1)\n",
    "\n",
    "def maxim(x):\n",
    "    return np.max(x, axis=-1)\n",
    "\n",
    "def argminim(x):\n",
    "    return np. argmin(x, axis=-1)\n",
    "\n",
    "def argmaxim(x):\n",
    "    return np.argmax(x,axis=-1)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1))\n",
    "\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1)\n",
    "\n",
    "def skewness(x):\n",
    "    return stats.skew(x, axis=-1)\n",
    "\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x, axis=-1)\n",
    "\n",
    "def concatenate_features(x):\n",
    "    return np.concatenate((mean(x), std(x), ptp(x), var(x), minim(x), maxim(x), argminim(x),\n",
    "                          argmaxim(x), rms(x), abs_diff_signal(x), skewness(x), kurtosis(x)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features=[]\n",
    "for d in data_array:\n",
    "    features.append(concatenate_features(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_array=np.array(features)\n",
    "features_array.shape # 1536 / 128 = 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# This section contains the function to support the model evaluation\n",
    "\n",
    "\n",
    "def run_cross_validation(classifier, x_tr, y_tr):\n",
    "    k_fold = model_selection.KFold(n_splits=10)\n",
    "    results = model_selection.cross_val_score(classifier, x_tr, y_tr, cv=k_fold, scoring='accuracy')\n",
    "    print('{:<50} {:.4f}'.format(\"Cross validation average accuracy with 10-fold:\", (results.mean())))\n",
    "\n",
    "def run_accuracy(y_tst, y_p):\n",
    "    print('{:<50} {:.4f}'.format(\"Accuracy\", (metrics.accuracy_score(y_tst, y_p))))\n",
    "\n",
    "def plot_confusion_matrix(y_tst, y_pred, y_labels):\n",
    "    lbs = y_labels.unique()\n",
    "    confusion_matrix = metrics.confusion_matrix(y_tst, y_pred)\n",
    "    matrix_df = pd.DataFrame(confusion_matrix)\n",
    "    ax = plt.axes()\n",
    "    sns.set(font_scale=1.3)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax, cmap=\"magma\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize =15)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=15)\n",
    "    ax.set_yticklabels(list(lbs), rotation = 0)\n",
    "    plt.show()\n",
    "\n",
    "# Extract importance\n",
    "def print_importance(classifier, x_tr):\n",
    "    importance = pd.DataFrame({'feature': x_tr.columns, 'importance' : np.round(classifier.feature_importances_, 3)})\n",
    "    importance.sort_values('importance', ascending=False, inplace = True)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy                                           0.2668\n",
      "Cross validation average accuracy with 10-fold:    0.2441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.21      0.23       183\n",
      "           1       0.32      0.32      0.32       169\n",
      "           2       0.23      0.30      0.26       156\n",
      "           3       0.26      0.25      0.25       163\n",
      "\n",
      "    accuracy                           0.27       671\n",
      "   macro avg       0.27      0.27      0.27       671\n",
      "weighted avg       0.27      0.27      0.27       671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n\u001b[1;32m     18\u001b[0m     evaluate_model(clf, x_train, x_test, y_train, y_test)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mrun_random_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mrun_random_forest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m run_cross_validation(clf, x_train, y_train)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n\u001b[0;32m---> 18\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(classifier, x_tr, x_tst, y_tr, y_tst)\u001b[0m\n\u001b[1;32m     47\u001b[0m t_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_tr)\n\u001b[1;32m     48\u001b[0m pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_tst)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mcalculate_and_plot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mcalculate_and_plot_model\u001b[0;34m(classifier, predictions, probabilities, train_predictions, train_probs, x_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_and_plot_model\u001b[39m(classifier, predictions, probabilities, train_predictions, train_probs, x_test, y_train, y_test):\n\u001b[1;32m     13\u001b[0m     baseline \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_test, [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test))], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(y_test, [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test))], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m}\n\u001b[1;32m     16\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_test, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(y_test, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m---> 17\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43movo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m}\n\u001b[1;32m     19\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_train, train_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     20\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(y_train, train_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     21\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m'\u001b[39m: roc_auc_score(y_train, train_probs, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#probability that given y value is 1\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:561\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    565\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:627\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m\"\"\"Multiclass roc auc score.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# validation of the input y_score\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[38;5;241m1\u001b[39m, \u001b[43my_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget scores need to be probabilities for multiclass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc, i.e. they should sum up to 1.0 over classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m     )\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# validation for multiclass parameter specifications\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_random_forest():\n",
    "    print(\"Random Forest\")\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_array, label_array, test_size=0.30) # 70% test and 30% training\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    run_accuracy(y_test, y_pred)\n",
    "    run_cross_validation(clf, x_train, y_train)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    evaluate_model(clf, x_train, x_test, y_train, y_test)\n",
    "\n",
    "run_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network\n",
      "Accuracy                                           0.2210\n",
      "Cross validation average accuracy with 10-fold:    0.2562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       117\n",
      "           1       0.00      0.00      0.00       125\n",
      "           2       0.00      0.00      0.00       107\n",
      "           3       0.22      1.00      0.36        99\n",
      "\n",
      "    accuracy                           0.22       448\n",
      "   macro avg       0.06      0.25      0.09       448\n",
      "weighted avg       0.05      0.22      0.08       448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valeria/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/valeria/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def run_neural_network():\n",
    "    print(\"Neural Network\")\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_array, label_array, test_size=0.20) # 80% test and 20% training\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    run_accuracy(y_test, y_pred)\n",
    "    run_cross_validation(clf, x_train, y_train)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(y_test, y_pred, label_array)\n",
    "\n",
    "run_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy                                           0.2522\n",
      "Cross validation average accuracy with 10-fold:    0.2344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.17      0.24       126\n",
      "           1       0.27      0.33      0.30       111\n",
      "           2       0.20      0.24      0.21       101\n",
      "           3       0.23      0.27      0.25       110\n",
      "\n",
      "    accuracy                           0.25       448\n",
      "   macro avg       0.27      0.25      0.25       448\n",
      "weighted avg       0.27      0.25      0.25       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiclass Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def run_random_forest_multi_class():\n",
    "    print(\"Random Forest\")\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_array, label_array, test_size=0.20) # 80% test and 20% training\n",
    "    rf = RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')\n",
    "    clf = OneVsRestClassifier(rf)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    run_accuracy(y_test, y_pred)\n",
    "    run_cross_validation(clf, x_train, y_train)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #print_importance(clf, x_train)\n",
    "    #plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "run_random_forest_multi_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy                                           0.2165\n",
      "Cross validation average accuracy with 10-fold:    0.2321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.16      0.19       122\n",
      "           1       0.24      0.27      0.25       102\n",
      "           2       0.20      0.19      0.19       117\n",
      "           3       0.20      0.26      0.23       107\n",
      "\n",
      "    accuracy                           0.22       448\n",
      "   macro avg       0.22      0.22      0.22       448\n",
      "weighted avg       0.22      0.22      0.21       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_random_forest(X, y):\n",
    "    print(\"Random Forest\")\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # 80% test and 20% training\n",
    "    clf = RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    run_accuracy(y_test, y_pred)\n",
    "    run_cross_validation(clf, x_train, y_train)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #print_importance(clf, x_train)\n",
    "    #plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "run_random_forest(features_array, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy                                           0.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/inner_speech/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_linear_svc_multi_class():\n",
    "    print(\"Linear SVC\")\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_array, label_array, test_size=0.30) # 70% test and 30% training\n",
    "    rf = LinearSVC(random_state=0, max_iter=10000)\n",
    "    clf = OneVsRestClassifier(rf)\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    run_accuracy(y_test, y_pred)\n",
    "    run_cross_validation(clf, x_train, y_train)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "run_linear_svc_multi_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
