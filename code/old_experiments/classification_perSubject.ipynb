{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Init libraries\n",
    "import warnings\n",
    "import mne\n",
    "import numpy as np\n",
    "from aux.pre_process import get_one_subject_data_and_label\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy import integrate\n",
    "from scipy import stats\n",
    "import antropy as ant\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(23)\n",
    "mne.set_log_level(verbose='warning') #to avoid info at terminal\n",
    "warnings.filterwarnings(action = \"ignore\", category = DeprecationWarning )\n",
    "warnings.filterwarnings(action = \"ignore\", category = FutureWarning )\n",
    "warnings.filterwarnings(action = \"ignore\", category = ConvergenceWarning )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define all the features\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    return np.mean(x, axis=-1)\n",
    "\n",
    "def std(x):\n",
    "    return np.std(x, axis=-1)\n",
    "\n",
    "def ptp(x):\n",
    "    return np.ptp(x, axis=-1)\n",
    "\n",
    "def var(x):\n",
    "    return np.var(x, axis=-1)\n",
    "\n",
    "def minim(x):\n",
    "    return np.min(x, axis=-1)\n",
    "\n",
    "def maxim(x):\n",
    "    return np.max(x, axis=-1)\n",
    "\n",
    "def argminim(x):\n",
    "    return np. argmin(x, axis=-1)\n",
    "\n",
    "def argmaxim(x):\n",
    "    return np.argmax(x,axis=-1)\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1))\n",
    "\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1)\n",
    "\n",
    "def skewness(x):\n",
    "    return stats.skew(x, axis=-1)\n",
    "\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x, axis=-1)\n",
    "\n",
    "def f_minplusmax(x):\n",
    "    return np.max(x, axis=-1) + np.min(x, axis=-1)\n",
    "\n",
    "def f_maxminusmin(x):\n",
    "    return np.max(x, axis=-1) - np.min(x, axis=-1)\n",
    "\n",
    "def f_spec_entropy(x):\n",
    "    return ant.spectral_entropy(x, fs, method=\"welch\", normalize=True, axis=-1)\n",
    "\n",
    "def f_integral(x):\n",
    "    return integrate.simps(x, axis=-1)\n",
    "\n",
    "def f_petrosian(x):\n",
    "    return ant.petrosian_fd(x, axis=-1)\n",
    "\n",
    "def f_katz(x):\n",
    "    return ant.katz_fd(x, axis=-1)\n",
    "\n",
    "def concatenate_features(x):\n",
    "    # Uncomment the desired line to add the feature\n",
    "    return np.concatenate((\n",
    "        mean(x),\n",
    "        std(x),\n",
    "        ptp(x),\n",
    "        var(x),\n",
    "        minim(x),\n",
    "        maxim(x),\n",
    "        argminim(x),\n",
    "        argmaxim(x),\n",
    "        rms(x),\n",
    "        abs_diff_signal(x),\n",
    "        skewness(x),\n",
    "        kurtosis(x),\n",
    "        # f_minplusmax(x),\n",
    "        # f_maxminusmin(x),\n",
    "        # f_spec_entropy(x),\n",
    "        # f_integral(x),\n",
    "        # f_katz(x),\n",
    "        # f_petrosian(x),\n",
    "    ), axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def run_cross_validation(classifier, k_fold, x_tr, y_tr):\n",
    "    # Changed to use StratifiedKFold\n",
    "    #k_fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    results = model_selection.cross_val_score(classifier, x_tr, y_tr, cv=k_fold, scoring='accuracy')\n",
    "    return results.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Project defaults\n",
    "# The root dir\n",
    "root_dir = \"./ds003626\"\n",
    "\n",
    "# Sampling rate\n",
    "fs = 256\n",
    "\n",
    "# Select the useful par of each trial. Time in seconds\n",
    "t_start = 1.5\n",
    "t_end = 3.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing subject: 1\n",
      "Old shape:  (200, 1536)\n",
      "New shape:  (200, 659)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2                  0.27222222222222225\n",
      "Neural Network                           0.3                  0.33333333333333337\n",
      "Linear SVC                               0.35                 0.3388888888888889\n",
      "SVC                                      0.45                 0.3555555555555555\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.125                0.25625        \n",
      "Neural Network                           0.3                  0.2875         \n",
      "Linear SVC                               0.325                0.2625         \n",
      "SVC                                      0.425                0.275          \n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.31666666666666665  0.2785714285714286\n",
      "Neural Network                           0.36666666666666664  0.3142857142857142\n",
      "Linear SVC                               0.35                 0.27142857142857146\n",
      "SVC                                      0.36666666666666664  0.3214285714285714\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 2\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 654)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3333333333333333   0.25940803382663846\n",
      "Neural Network                           0.3333333333333333   0.3659619450317125\n",
      "Linear SVC                               0.375                0.32854122621564485\n",
      "SVC                                      0.5416666666666666   0.35200845665961944\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3125               0.29176788124156544\n",
      "Neural Network                           0.3125               0.33859649122807023\n",
      "Linear SVC                               0.3333333333333333   0.3854251012145749\n",
      "SVC                                      0.4791666666666667   0.3643724696356275\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2222222222222222   0.3153297682709447\n",
      "Neural Network                           0.2916666666666667   0.35062388591800353\n",
      "Linear SVC                               0.3055555555555556   0.39233511586452763\n",
      "SVC                                      0.3194444444444444   0.37415329768270944\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 3\n",
      "Old shape:  (180, 1536)\n",
      "New shape:  (180, 644)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.16666666666666666  0.28428030303030305\n",
      "Neural Network                           0.2777777777777778   0.38371212121212117\n",
      "Linear SVC                               0.2222222222222222   0.3958333333333333\n",
      "SVC                                      0.16666666666666666  0.4456439393939394\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3333333333333333   0.27881773399014775\n",
      "Neural Network                           0.4444444444444444   0.3258620689655173\n",
      "Linear SVC                               0.3888888888888889   0.3613300492610837\n",
      "SVC                                      0.3888888888888889   0.4093596059113301\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2222222222222222   0.29292307692307695\n",
      "Neural Network                           0.37037037037037035  0.3412307692307692\n",
      "Linear SVC                               0.3333333333333333   0.3572307692307692\n",
      "SVC                                      0.37037037037037035  0.34092307692307694\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 4\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 653)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.16666666666666666  0.19038054968287527\n",
      "Neural Network                           0.3333333333333333   0.3059196617336152\n",
      "Linear SVC                               0.2916666666666667   0.25961945031712474\n",
      "SVC                                      0.5                  0.28699788583509517\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.20833333333333334  0.22941970310391363\n",
      "Neural Network                           0.14583333333333334  0.2191632928475034\n",
      "Linear SVC                               0.14583333333333334  0.25020242914979757\n",
      "SVC                                      0.22916666666666666  0.26059379217273954\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2222222222222222   0.22014260249554368\n",
      "Neural Network                           0.2222222222222222   0.20249554367201425\n",
      "Linear SVC                               0.25                 0.23262032085561496\n",
      "SVC                                      0.2638888888888889   0.2737967914438503\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 5\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 577)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2916666666666667   0.18551797040169132\n",
      "Neural Network                           0.3333333333333333   0.33350951374207194\n",
      "Linear SVC                               0.375                0.26839323467230447\n",
      "SVC                                      0.4583333333333333   0.28731501057082454\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.20833333333333334  0.2241565452091768\n",
      "Neural Network                           0.3125               0.2812415654520918\n",
      "Linear SVC                               0.2708333333333333   0.32388663967611336\n",
      "SVC                                      0.3958333333333333   0.34426450742240217\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3055555555555556   0.2855614973262032\n",
      "Neural Network                           0.2638888888888889   0.27415329768270946\n",
      "Linear SVC                               0.2638888888888889   0.27415329768270946\n",
      "SVC                                      0.2638888888888889   0.2855614973262032\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 6\n",
      "Old shape:  (216, 1536)\n",
      "New shape:  (216, 660)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.36363636363636365  0.2786774628879892\n",
      "Neural Network                           0.45454545454545453  0.4481781376518219\n",
      "Linear SVC                               0.5                  0.4278002699055331\n",
      "SVC                                      0.45454545454545453  0.47381916329284757\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3181818181818182   0.267563025210084\n",
      "Neural Network                           0.36363636363636365  0.4189915966386555\n",
      "Linear SVC                               0.38636363636363635  0.46436974789915964\n",
      "SVC                                      0.38636363636363635  0.4700840336134453\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3230769230769231   0.2182795698924731\n",
      "Neural Network                           0.4307692307692308   0.370752688172043\n",
      "Linear SVC                               0.4                  0.4178494623655914\n",
      "SVC                                      0.4307692307692308   0.43139784946236565\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 7\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 650)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.16666666666666666  0.22177589852008456\n",
      "Neural Network                           0.4166666666666667   0.27346723044397464\n",
      "Linear SVC                               0.4166666666666667   0.30517970401691336\n",
      "SVC                                      0.4583333333333333   0.295877378435518\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.20833333333333334  0.27139001349527664\n",
      "Neural Network                           0.3125               0.3441295546558704\n",
      "Linear SVC                               0.2708333333333333   0.30269905533063424\n",
      "SVC                                      0.3125               0.30269905533063424\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.2222222222222222   0.27397504456327987\n",
      "Neural Network                           0.2361111111111111   0.28627450980392155\n",
      "Linear SVC                               0.2638888888888889   0.29251336898395724\n",
      "SVC                                      0.2916666666666667   0.3155080213903743\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 8\n",
      "Old shape:  (200, 1536)\n",
      "New shape:  (200, 611)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.4                  0.2611111111111112\n",
      "Neural Network                           0.25                 0.31666666666666665\n",
      "Linear SVC                               0.4                  0.3388888888888889\n",
      "SVC                                      0.35                 0.3555555555555555\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.35                 0.2625         \n",
      "Neural Network                           0.2                  0.3125         \n",
      "Linear SVC                               0.25                 0.31875        \n",
      "SVC                                      0.325                0.30625        \n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.23333333333333334  0.27142857142857146\n",
      "Neural Network                           0.23333333333333334  0.3            \n",
      "Linear SVC                               0.2833333333333333   0.3071428571428571\n",
      "SVC                                      0.3                  0.3357142857142857\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 9\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 632)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.20833333333333334  0.2862579281183932\n",
      "Neural Network                           0.25                 0.33298097251585623\n",
      "Linear SVC                               0.3333333333333333   0.36553911205073997\n",
      "SVC                                      0.4166666666666667   0.34249471458773784\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.20833333333333334  0.21268556005398107\n",
      "Neural Network                           0.22916666666666666  0.27071524966261806\n",
      "Linear SVC                               0.3125               0.2757085020242915\n",
      "SVC                                      0.3333333333333333   0.28137651821862353\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.16666666666666666  0.23868092691622103\n",
      "Neural Network                           0.2361111111111111   0.33957219251336895\n",
      "Linear SVC                               0.2638888888888889   0.30409982174688055\n",
      "SVC                                      0.2916666666666667   0.39893048128342246\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "Doing subject: 10\n",
      "Old shape:  (240, 1536)\n",
      "New shape:  (240, 610)\n",
      "\n",
      "Split: Train:90.0% Test:10.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3333333333333333   0.24989429175475686\n",
      "Neural Network                           0.3333333333333333   0.3379492600422833\n",
      "Linear SVC                               0.2916666666666667   0.38002114164904865\n",
      "SVC                                      0.25                 0.39397463002114164\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.25                 0.27112010796221325\n",
      "Neural Network                           0.2916666666666667   0.30175438596491233\n",
      "Linear SVC                               0.3541666666666667   0.29730094466936574\n",
      "SVC                                      0.2916666666666667   0.39109311740890684\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Classifier                               Accuracy             Cross validation\n",
      "Random Forest                            0.3055555555555556   0.3213903743315508\n",
      "Neural Network                           0.4166666666666667   0.3037433155080214\n",
      "Linear SVC                               0.4305555555555556   0.3213903743315508\n",
      "SVC                                      0.4722222222222222   0.3449197860962567\n",
      "******************************************************************\n",
      "******************************************************************\n",
      "******************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "\n",
    "subjects = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "random_state = 42\n",
    "splits = [0.10, 0.20, 0.30]\n",
    "\n",
    "for subject in subjects:\n",
    "\n",
    "    # The root dir\n",
    "    root_dir = \"./ds003626\"\n",
    "\n",
    "    # Sampling rate\n",
    "    fs = 256\n",
    "\n",
    "    # Select the useful par of each trial. Time in seconds\n",
    "    t_start = 1.5\n",
    "    t_end = 3.5\n",
    "\n",
    "    print(f\"Doing subject: {subject}\")\n",
    "    condition = \"Inner\"\n",
    "    data, labels = get_one_subject_data_and_label(root_dir, subject, condition, t_start = t_start, t_end = t_end, fs = fs)\n",
    "\n",
    "    #convert from list to array\n",
    "    data_array=np.vstack(data)\n",
    "    label_array=np.hstack(labels)\n",
    "\n",
    "    # Make features\n",
    "    features=[]\n",
    "    for d in data_array:\n",
    "     features.append(concatenate_features(d))\n",
    "    features_array=np.array(features)\n",
    "\n",
    "    # Run the experiment\n",
    "\n",
    "    # Prepare the data\n",
    "    X = features_array\n",
    "    y = label_array\n",
    "    #X = MinMaxScaler().fit_transform(X)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Feature Selection - There are other parameters we could set for Feature Selection\n",
    "    print(\"Old shape: \", X.shape)\n",
    "\n",
    "    # Select one fs here\n",
    "    fs = LinearSVC(C=0.01, penalty=\"l2\", dual=False).fit(X, y)\n",
    "    # fs = SVC(kernel=\"linear\").fit(X, y)\n",
    "    # fs = ExtraTreesClassifier(n_estimators=50).fit(X, y)\n",
    "\n",
    "    model = SelectFromModel(fs, prefit=True)\n",
    "    X = model.transform(X)\n",
    "\n",
    "    # or use this one\n",
    "    # X = SelectKBest(chi2, k=100).fit_transform(X, y)\n",
    "\n",
    "    print(\"New shape: \", X.shape)\n",
    "\n",
    "    # Run cross validation with best hyperparameters found from all subjects\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    classifiers = [\n",
    "        [\"Random Forest\", RandomForestClassifier(random_state=random_state, max_features='log2', n_estimators= 200, max_depth=7, criterion='entropy')],\n",
    "        [\"Neural Network\", MLPClassifier(random_state=random_state, alpha=1e-06, hidden_layer_sizes=11, max_iter=2000, solver='lbfgs')],\n",
    "        [\"Linear SVC\", LinearSVC(random_state=random_state, max_iter=10000, C=0.0005)],\n",
    "        [\"SVC\", SVC(random_state=random_state, max_iter=10000, C=10, kernel='linear')],\n",
    "    ]\n",
    "\n",
    "    for test_size in splits:\n",
    "        print(\"\\nSplit: Train:{}% Test:{}%\".format(100 - (test_size * 100), test_size * 100))\n",
    "        print('{:<40} {:<20} {:<15}'.format(\"Classifier\", \"Accuracy\", \"Cross validation\"))\n",
    "\n",
    "        # Stratify guarantees that the same proportion of the classes will be available in train and test\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n",
    "\n",
    "        for cls in classifiers:\n",
    "            cls[1].fit(x_train, y_train)\n",
    "            y_pred = cls[1].predict(x_test)\n",
    "            accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "            cross_v = run_cross_validation(cls[1], outer_cv, x_train, y_train)\n",
    "            print('{:<40} {:<20} {:<15}'.format(cls[0], accuracy, cross_v))\n",
    "    print('******************************************************************')\n",
    "    print('******************************************************************')\n",
    "    print('******************************************************************')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.2638888888888889, 0.25]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implementing Dummy Classifier to have a baseline for our models\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strategies = ['most_frequent', 'stratified', 'uniform']\n",
    "\n",
    "test_scores = []\n",
    "for s in strategies:\n",
    "    dclf = DummyClassifier(strategy = s, random_state = random_state)\n",
    "    dclf.fit(x_train, y_train)\n",
    "    score = dclf.score(x_test, y_test)\n",
    "    test_scores.append(score)\n",
    "\n",
    "print(test_scores)\n",
    "ax = sns.stripplot(strategies, test_scores);\n",
    "ax.set(xlabel ='Strategy', ylabel ='Test Score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#Run until here for now, the rest of the code bellow hasn't been run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_result(history):\n",
    "    rcParams['figure.figsize'] = (18, 8)\n",
    "    rcParams['axes.spines.top'] = False\n",
    "    rcParams['axes.spines.right'] = False\n",
    "\n",
    "    plt.plot(\n",
    "    np.arange(1, 101),\n",
    "        history.history['loss'], label='Loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(1, 101),\n",
    "        history.history['accuracy'], label='Accuracy'\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(1, 101),\n",
    "        history.history['precision'], label='Precision'\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(1, 101),\n",
    "        history.history['recall'], label='Recall'\n",
    "    )\n",
    "    plt.title('Evaluation metrics', size=20)\n",
    "    plt.xlabel('Epoch', size=14)\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 09:28:47.628447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-08 09:28:56.415991: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-08 09:28:56.481437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-08 09:28:56.700854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:56.701487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-07-08 09:28:56.701831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-08 09:28:56.939251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-08 09:28:56.939696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-08 09:28:57.083140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-08 09:28:57.137688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-08 09:28:57.393263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-08 09:28:57.439753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-08 09:28:58.069974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-08 09:28:58.070648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:58.071353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:58.071530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-08 09:28:58.072556: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-08 09:28:58.074962: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-08 09:28:58.075457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:58.075663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-07-08 09:28:58.075900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-08 09:28:58.076044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-08 09:28:58.076132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-08 09:28:58.076184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-08 09:28:58.076215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-08 09:28:58.076236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-08 09:28:58.076255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-08 09:28:58.076273: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-08 09:28:58.076504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:58.076848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:28:58.076981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-08 09:28:58.082852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-08 09:34:01.704551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-08 09:34:01.704568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-08 09:34:01.704573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-08 09:34:01.704841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:34:01.705017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:34:01.705150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 09:34:01.705240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8751 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split: Train:90.0% Test:10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 09:34:01.986345: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-08 09:34:02.056897: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3692895000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 09:34:02.942981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 69s 2ms/step - loss: -953.5676 - accuracy: 0.2591 - precision: 0.8116 - recall: 0.7191\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -106098.3281 - accuracy: 0.2159 - precision: 0.7469 - recall: 1.0000\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1313672.0469 - accuracy: 0.2676 - precision: 0.7601 - recall: 1.0000\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -7325321.6875 - accuracy: 0.2526 - precision: 0.7510 - recall: 1.0000\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -30102331.5000 - accuracy: 0.2349 - precision: 0.7389 - recall: 1.0000\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -73237909.7500 - accuracy: 0.2577 - precision: 0.7217 - recall: 1.0000\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -203070954.0000 - accuracy: 0.2431 - precision: 0.7271 - recall: 1.0000\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -502926104.0000 - accuracy: 0.2331 - precision: 0.7587 - recall: 1.0000\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -901204488.0000 - accuracy: 0.2321 - precision: 0.7495 - recall: 1.0000\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1576276456.0000 - accuracy: 0.2715 - precision: 0.7409 - recall: 1.0000\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2928404448.0000 - accuracy: 0.2326 - precision: 0.7308 - recall: 1.0000\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -5740507456.0000 - accuracy: 0.2006 - precision: 0.7462 - recall: 1.0000\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -9045028032.0000 - accuracy: 0.2453 - precision: 0.8007 - recall: 1.0000\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -13189534336.0000 - accuracy: 0.2156 - precision: 0.7379 - recall: 1.0000\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -20866799872.0000 - accuracy: 0.2771 - precision: 0.7733 - recall: 1.0000\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -31701216000.0000 - accuracy: 0.2359 - precision: 0.7424 - recall: 1.0000\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -29829815040.0000 - accuracy: 0.2908 - precision: 0.7232 - recall: 1.0000\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -51272248320.0000 - accuracy: 0.2615 - precision: 0.7493 - recall: 1.0000\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -79377165312.0000 - accuracy: 0.2359 - precision: 0.7420 - recall: 1.0000\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -86025148416.0000 - accuracy: 0.2615 - precision: 0.7189 - recall: 1.0000\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -129832537088.0000 - accuracy: 0.1908 - precision: 0.7304 - recall: 1.0000\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -163298246656.0000 - accuracy: 0.2476 - precision: 0.7572 - recall: 1.0000\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -224461395968.0000 - accuracy: 0.2800 - precision: 0.7749 - recall: 1.0000\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -223863199744.0000 - accuracy: 0.2435 - precision: 0.7470 - recall: 1.0000\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -326894145536.0000 - accuracy: 0.2717 - precision: 0.7781 - recall: 1.0000\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -385457901568.0000 - accuracy: 0.2419 - precision: 0.7364 - recall: 1.0000\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -464184262656.0000 - accuracy: 0.2429 - precision: 0.7451 - recall: 1.0000\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -644804001792.0000 - accuracy: 0.2454 - precision: 0.7698 - recall: 1.0000\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -706979471360.0000 - accuracy: 0.2378 - precision: 0.7303 - recall: 1.0000\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -739763732480.0000 - accuracy: 0.3043 - precision: 0.7593 - recall: 1.0000\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1022197678080.0000 - accuracy: 0.2575 - precision: 0.7476 - recall: 1.0000\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1173322579968.0000 - accuracy: 0.2164 - precision: 0.7210 - recall: 1.0000\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1326038204416.0000 - accuracy: 0.2600 - precision: 0.7469 - recall: 1.0000\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1789061136384.0000 - accuracy: 0.2305 - precision: 0.7514 - recall: 1.0000\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2116950540288.0000 - accuracy: 0.2613 - precision: 0.7626 - recall: 1.0000\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -1890989457408.0000 - accuracy: 0.2641 - precision: 0.7417 - recall: 1.0000\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2249794666496.0000 - accuracy: 0.2439 - precision: 0.7273 - recall: 1.0000\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2707221774336.0000 - accuracy: 0.2568 - precision: 0.7496 - recall: 1.0000\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2878321721344.0000 - accuracy: 0.2545 - precision: 0.7662 - recall: 1.0000\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -2760436461568.0000 - accuracy: 0.2879 - precision: 0.7262 - recall: 1.0000\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -3828303495168.0000 - accuracy: 0.2501 - precision: 0.7622 - recall: 1.0000\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -4188088860672.0000 - accuracy: 0.2407 - precision: 0.7239 - recall: 1.0000\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -5802794680320.0000 - accuracy: 0.2420 - precision: 0.7608 - recall: 1.0000\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -5499359264768.0000 - accuracy: 0.2574 - precision: 0.7609 - recall: 1.0000\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -5215090409472.0000 - accuracy: 0.2700 - precision: 0.7280 - recall: 1.0000\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -7216671293440.0000 - accuracy: 0.2119 - precision: 0.7222 - recall: 1.0000\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -9311714148352.0000 - accuracy: 0.2465 - precision: 0.7797 - recall: 1.0000\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -7842221948928.0000 - accuracy: 0.2431 - precision: 0.7408 - recall: 1.0000\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -8974689173504.0000 - accuracy: 0.2389 - precision: 0.7495 - recall: 1.0000\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -9753404243968.0000 - accuracy: 0.2384 - precision: 0.7686 - recall: 1.0000\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -9822191616000.0000 - accuracy: 0.2822 - precision: 0.7594 - recall: 1.0000\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -12610540535808.0000 - accuracy: 0.2333 - precision: 0.7521 - recall: 1.0000\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -13019903950848.0000 - accuracy: 0.2479 - precision: 0.7612 - recall: 1.0000\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -12676574216192.0000 - accuracy: 0.2081 - precision: 0.7374 - recall: 1.0000\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -15183324905472.0000 - accuracy: 0.2372 - precision: 0.7342 - recall: 1.0000\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -18448811819008.0000 - accuracy: 0.2267 - precision: 0.7546 - recall: 1.0000\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -17871698657280.0000 - accuracy: 0.3082 - precision: 0.7771 - recall: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -20586220224512.0000 - accuracy: 0.2182 - precision: 0.7505 - recall: 1.0000\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -23355721777152.0000 - accuracy: 0.2402 - precision: 0.7686 - recall: 1.0000\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -24242415468544.0000 - accuracy: 0.2320 - precision: 0.7351 - recall: 1.0000\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -22329234030592.0000 - accuracy: 0.2583 - precision: 0.7305 - recall: 1.0000\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: -22188358467584.0000 - accuracy: 0.2290 - precision: 0.7116 - recall: 1.0000\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -33190184484864.0000 - accuracy: 0.2176 - precision: 0.7740 - recall: 1.0000\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -28854294151168.0000 - accuracy: 0.2684 - precision: 0.7363 - recall: 1.0000\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -31860386430976.0000 - accuracy: 0.2393 - precision: 0.7618 - recall: 1.0000\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -29091850354688.0000 - accuracy: 0.2137 - precision: 0.7167 - recall: 1.0000\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -36714418536448.0000 - accuracy: 0.2539 - precision: 0.7668 - recall: 1.0000\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -46898641633280.0000 - accuracy: 0.2175 - precision: 0.7907 - recall: 1.0000\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -36631753523200.0000 - accuracy: 0.2649 - precision: 0.7390 - recall: 1.0000\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -44520950988800.0000 - accuracy: 0.2331 - precision: 0.7208 - recall: 1.0000\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -46310112100352.0000 - accuracy: 0.2856 - precision: 0.7992 - recall: 1.0000\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -47328651640832.0000 - accuracy: 0.2650 - precision: 0.7482 - recall: 1.0000\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -43475284787200.0000 - accuracy: 0.2460 - precision: 0.7310 - recall: 1.0000\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -56240557785088.0000 - accuracy: 0.2585 - precision: 0.7344 - recall: 1.0000\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -59946135715840.0000 - accuracy: 0.2260 - precision: 0.7424 - recall: 1.0000\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -61362406424576.0000 - accuracy: 0.2896 - precision: 0.7915 - recall: 1.0000\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -63581496803328.0000 - accuracy: 0.2422 - precision: 0.7510 - recall: 1.0000\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -72266425565184.0000 - accuracy: 0.2309 - precision: 0.7496 - recall: 1.0000\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -70891508596736.0000 - accuracy: 0.2745 - precision: 0.7538 - recall: 1.0000\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -69193562062848.0000 - accuracy: 0.2614 - precision: 0.7613 - recall: 1.0000\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -75864568496128.0000 - accuracy: 0.2633 - precision: 0.7503 - recall: 1.0000\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -86465667137536.0000 - accuracy: 0.2424 - precision: 0.7364 - recall: 1.0000\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -88721884971008.0000 - accuracy: 0.2317 - precision: 0.7570 - recall: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -103534000340992.0000 - accuracy: 0.2360 - precision: 0.7479 - recall: 1.0000\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -88587552948224.0000 - accuracy: 0.2454 - precision: 0.7449 - recall: 1.0000\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -105540947542016.0000 - accuracy: 0.2456 - precision: 0.7392 - recall: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -113396198408192.0000 - accuracy: 0.2482 - precision: 0.7376 - recall: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -111707506606080.0000 - accuracy: 0.2652 - precision: 0.7781 - recall: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -138981094195200.0000 - accuracy: 0.2319 - precision: 0.7316 - recall: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -117532999548928.0000 - accuracy: 0.2568 - precision: 0.7118 - recall: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -139798741254144.0000 - accuracy: 0.2051 - precision: 0.7457 - recall: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -151103107760128.0000 - accuracy: 0.2686 - precision: 0.7720 - recall: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -165903715008512.0000 - accuracy: 0.2139 - precision: 0.7471 - recall: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -156135365541888.0000 - accuracy: 0.2582 - precision: 0.7606 - recall: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -130399834472448.0000 - accuracy: 0.2848 - precision: 0.7371 - recall: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -168229320261632.0000 - accuracy: 0.2557 - precision: 0.7482 - recall: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -185546154442752.0000 - accuracy: 0.2891 - precision: 0.7676 - recall: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -180367461974016.0000 - accuracy: 0.2277 - precision: 0.7376 - recall: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -189233014767616.0000 - accuracy: 0.2416 - precision: 0.7308 - recall: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: -193121570783232.0000 - accuracy: 0.2568 - precision: 0.7402 - recall: 1.0000\n",
      "\n",
      "Split: Train:80.0% Test:20.0%\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -192529140023296.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -199281164156928.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -205637514428416.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -212817324015616.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -220378345504768.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -227177211625472.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -234957310001152.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -243336153661440.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -250664139620352.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -259339453464576.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -267151747317760.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -275873248837632.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -284873940008960.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -293797674090496.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -302553166249984.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -312169061154816.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -321584065675264.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -331260425666560.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -342566692192256.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -352170977263616.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -362534431358976.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -373573201952768.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -384600396267520.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -396189258219520.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -407316981612544.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -419798425010176.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -431331116843008.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -443215928885248.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -456463721955328.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -468919261331456.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -482079510888448.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -496539659141120.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -508564795817984.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -522914147336192.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -537795772809216.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -551598187085824.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -566521822707712.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -581560248041472.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -596907038605312.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -612732650913792.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -628209230020608.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -644524602818560.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -660707100065792.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -677134142013440.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -693808278798336.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -712586882449408.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -729828760223744.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -747784407875584.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -766575091122176.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -784126877630464.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -804013918388224.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -822168141168640.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -842399853051904.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -863238027739136.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -883916885983232.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -904258656403456.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -924024163008512.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -947305771433984.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -967911212580864.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -990249538813952.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1010960072441856.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1034976455819264.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1059138599649280.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1081948097216512.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1105485591740416.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1129223540441088.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1156158890967040.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1180705165934592.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1203246496481280.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1229049452036096.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1256257667203072.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1282351472574464.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1310248627339264.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1336532216578048.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1364780887572480.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1392796153937920.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1420617207250944.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1450981686509568.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1478769453826048.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1507795513901056.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1539049990914048.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1571813477842944.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1600039734476800.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1632369027055616.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1664706104262656.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1695605978038272.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1727356523773952.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1764590669004800.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1795707035975680.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1831182794752000.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1865776071966720.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1898787190603776.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1934444277530624.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -1972670291771392.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2005895085031424.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2043282775343104.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2082535018332160.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2119356578267136.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2155716966088704.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2196179316113408.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "\n",
      "Split: Train:70.0% Test:30.0%\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2290104747950080.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2332167510163456.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2372363303780352.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2417672524398592.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2463351649075200.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2504399188393984.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2554062968979456.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2598559635472384.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2644630944350208.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2689495098982400.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2739357253369856.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2784665668681728.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2836126624645120.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2883172354228224.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2934791955546112.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -2982844854960128.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3032565812297728.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3080096604749824.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3136984520327168.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3194025980985344.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3244364608307200.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3299674391838720.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3353686021505024.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3406948984684544.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3465242696744960.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3523175631552512.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3578106921091072.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3641516744507392.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3702349050675200.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3762613078982656.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3817426525356032.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3871318164373504.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3932899137028096.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -3988542317395968.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4049441732427776.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4109606439616512.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4171141241372672.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4237185221918720.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4298409443852288.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4367114693509120.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4431778378940416.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4497020542779392.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4556738573369344.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4623742747541504.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4693544556036096.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4763291603697664.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4829297734844416.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4905520519446528.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -4973741243105280.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5043780348542976.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5112235147919360.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5173244084617216.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5255280107454464.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5323675314159616.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5397813429010432.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5471669149761536.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5543970629222400.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5622340897472512.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5697567249661952.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5777085884792832.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5853168546086912.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -5933890308931584.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6011295820152832.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6092023488577536.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6166579825868800.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6248941192478720.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6331418523205632.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6413096990015488.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6498815947309056.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6579033588367360.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6670462704680960.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6764184796659712.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6850076626386944.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -6931222517252096.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7019648914554880.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7101670441877504.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7194429387440128.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7277036104056832.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7370010334855168.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7456220529033216.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7551022805286912.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7647885592100864.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7741016589205504.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7837971717816320.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -7934277769494528.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8027997177118720.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8124494354841600.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8217692460810240.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8310527642042368.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8399597210697728.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8500781808353280.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8595649045987328.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8698347082743808.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: -8799352365514752.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -8904510881660928.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -9002277759090688.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -9106143490080768.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -9215854134689792.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -9323087656910848.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: -9425339017068544.0000 - accuracy: 0.2500 - precision: 0.7500 - recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1296x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import rcParams\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.03),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "for test_size in splits:\n",
    "    print(\"\\nSplit: Train:{}% Test:{}%\".format(100 - (test_size * 100), test_size * 100))\n",
    "\n",
    "    # Stratify guarantees that the same proportion of the classes will be available in train and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n",
    "    history = model.fit(x_train, y_train, epochs=100)\n",
    "    plot_result(history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (192, 128, 512)\n",
      "192 train samples\n",
      "X_test shape: (48, 128, 512)\n",
      "48 test samples\n",
      "Y_train shape: (192, 4)\n",
      "192 train samples\n",
      "Y_test shape: (48, 4)\n",
      "48 test samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 09:35:17.163348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-08 09:45:46.594841: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2022-07-08 09:45:46.629910: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 632s - loss: nan - accuracy: 0.2500\n",
      "Epoch 2/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 3/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 4/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 5/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 6/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 7/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 8/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 9/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 10/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 11/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 12/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 13/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 14/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 15/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 16/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 17/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 18/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 19/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 20/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 21/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 22/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 23/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 24/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 25/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 26/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 27/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 28/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 29/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 30/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 31/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 32/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 33/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 34/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 35/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 36/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 37/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 38/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 39/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 40/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 41/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 42/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 43/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 44/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 45/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 46/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 47/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 48/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 49/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 50/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 51/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 52/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 53/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 54/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 55/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 56/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 57/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 58/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 59/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 60/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 61/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 62/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 63/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 64/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 65/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 66/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 67/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 68/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 69/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 70/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 71/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 72/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 73/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 74/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 75/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 76/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 77/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 78/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 79/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 80/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 81/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 82/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 83/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 84/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 85/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 86/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 87/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 88/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 89/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 90/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 91/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 92/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 93/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 94/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 95/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 96/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 97/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 98/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 99/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 100/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 101/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 102/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 103/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 104/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 105/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 106/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 107/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 108/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 109/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 110/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 111/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 112/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 113/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 114/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 115/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 116/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 117/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 118/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 119/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 120/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 121/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 122/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 123/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 124/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 125/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 126/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 127/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 128/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 129/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 130/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 131/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 132/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 133/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 134/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 135/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 136/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 137/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 138/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 139/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 140/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 141/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 142/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 143/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 144/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 145/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 146/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 147/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 148/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 149/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 150/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 151/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 152/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 153/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 154/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 155/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 156/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 157/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 158/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 159/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 160/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 161/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 162/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 163/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 164/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 165/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 166/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 167/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 168/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 169/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 170/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 171/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 172/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 173/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 174/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 175/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 176/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 177/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 178/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 179/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 180/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 181/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 182/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 183/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 184/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 185/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 186/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 187/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 188/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 189/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 190/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 191/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 192/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 193/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 194/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 195/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 196/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 197/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 198/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 199/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 200/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 201/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 202/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 203/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 204/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 205/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 206/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 207/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 208/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 209/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 210/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 211/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 212/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 213/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 214/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 215/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 216/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 217/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 218/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 219/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 220/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 221/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 222/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 223/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 224/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 225/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 226/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 227/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 228/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 229/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 230/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 231/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 232/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 233/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 234/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 235/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 236/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 237/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 238/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 239/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 240/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 241/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 242/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 243/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 244/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 245/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 246/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 247/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 248/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 249/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 250/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 251/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 252/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 253/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 254/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 255/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 256/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 257/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 258/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 259/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 260/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 261/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 262/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 263/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 264/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 265/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 266/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 267/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 268/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 269/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 270/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 271/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 272/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 273/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 274/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 275/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 276/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 277/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 278/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 279/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 280/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 281/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 282/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 283/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 284/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 285/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 286/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 287/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 288/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 289/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 290/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 291/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 292/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 293/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 294/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 295/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 296/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 297/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 298/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 299/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n",
      "Epoch 300/300\n",
      "12/12 - 0s - loss: nan - accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from others.eeg import EEGNet\n",
    "\n",
    "def run_eeg():\n",
    "    model = EEGNet(nb_classes = 4, Chans = 128, Samples = 512)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "    Xx = data_array\n",
    "    Yy = label_array\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_array, label_array, test_size=0.20, stratify=y)\n",
    "\n",
    "    # convert labels to one-hot encodings.\n",
    "    Y_train = to_categorical(y_train)\n",
    "    Y_test = to_categorical(y_test)\n",
    "\n",
    "    X_train = x_train.reshape(x_train.shape[0], 128, 512)\n",
    "    X_test = x_test.reshape(x_test.shape[0], 128, 512)\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    print('Y_train shape:', Y_train.shape)\n",
    "    print(Y_train.shape[0], 'train samples')\n",
    "    print('Y_test shape:', Y_test.shape)\n",
    "    print(Y_test.shape[0], 'test samples')\n",
    "\n",
    "    # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other\n",
    "    # model configurations may do better, but this is a good starting point)\n",
    "    model = EEGNet(nb_classes = 4, Chans = 128, Samples = 512)\n",
    "\n",
    "    # compile the model and set the optimizers\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    # count number of parameters in the model\n",
    "    numParams    = model.count_params()\n",
    "\n",
    "    fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, verbose = 2)\n",
    "\n",
    "run_eeg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}